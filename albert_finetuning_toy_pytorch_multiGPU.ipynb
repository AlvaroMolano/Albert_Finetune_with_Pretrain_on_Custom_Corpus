{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Albert with downstream tasks\n",
    "#### Overview\n",
    "This notebook illustrates finetuning the pretrained albert model on custom corpus, with an example of token classification task. \n",
    "The notebook uses transformers repo from Huggingface team and implements in Pytorch\n",
    "#### Steps\n",
    "1. Clone transformers repo from Huggingface team\n",
    "2. Install required dependencies as per transformers repo\n",
    "3. create this notebook in the transformers/src folder\n",
    "\n",
    "### Note\n",
    "Below notebook didnt specifically test on the toy data, but it was tested with a real application data, for a similar token classification purpose of the same input format. For your reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of writing, Hugging face didnt provide the class object for \n",
    "# AlbertForTokenClassification, hence write your own defination below\n",
    "from transformers.modeling_albert import AlbertModel, load_tf_weights_in_albert, AlbertPreTrainedModel\n",
    "from transformers.configuration_albert import AlbertConfig\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class AlbertForTokenClassification(AlbertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, albert, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.albert = albert\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.albert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenizer, load Albert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = \"models_toy/vocab.txt\" # This is the vocab file output from Build Vocab step\n",
    "CONFIG_FILE = \"models_toy/albert_config.json\"\n",
    "ALBERT_PRETRAIN_CHECKPOINT = \"/models/model.ckpt\" # This is the model checkpoint output from Albert Pretrain step\n",
    "tokenizer = BertTokenizer(vocab_file=VOCAB_FILE)\n",
    "config = AlbertConfig.from_json_file(CONFIG_FILE)\n",
    "model = AlbertModel(config)\n",
    "model = load_tf_weights_in_albert(model, config,ALBERT_PRETRAIN_CHECKPOINT)\n",
    "# If the variables not able to be initialized are only for the MLM and sequence order prediction task\n",
    "# Then the error could be ignored\n",
    "# As that is not required for the AlbertForTokenClassification we are trying to build here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input files and create input torch tensors, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data_toy/dish_name_train.csv\"\n",
    "EVAL_FILE = \"data_toy/dish_name_val.csv\"\n",
    "\n",
    "import numpy as np\n",
    "def label_sent(name_tokens, sent_tokens):\n",
    "    label = []\n",
    "    i = 0\n",
    "    if len(name_tokens)>len(sent_tokens):\n",
    "        label = np.zeros(len(sent_tokens))\n",
    "    else:\n",
    "        while i<len(sent_tokens):\n",
    "            found_match = False\n",
    "            if name_tokens[0] == sent_tokens[i]:       \n",
    "                found_match = True\n",
    "                for j in range(len(name_tokens)-1):\n",
    "                    if ((i+j+1)>=len(sent_tokens)):\n",
    "                        return label\n",
    "                    if name_tokens[j+1] != sent_tokens[i+j+1]:\n",
    "                        found_match = False\n",
    "                if found_match:\n",
    "                    label.extend(list(np.ones(len(name_tokens)).astype(int)))\n",
    "                    i = i + len(name_tokens)\n",
    "                else: \n",
    "                    label.extend([0])\n",
    "                    i = i+ 1\n",
    "            else:\n",
    "                label.extend([0])\n",
    "                i=i+1\n",
    "    return label\n",
    "\n",
    "import pandas as pd\n",
    "df_data_train = pd.read_csv(TRAIN_FILE)\n",
    "df_data_train['review_tokens'] = df_data_train.review.apply(tokenizer.tokenize)\n",
    "df_data_train['dish_name_tokens'] = df_data_train.dish_name_tokens.apply(tokenizer.tokenize)\n",
    "df_data_train['review_labels'] = df_data_train.apply(lambda row: label_sent(row['dish_name_tokens'] row['review_tokens']), axis=1)\n",
    "\n",
    "df_data_val = pd.read_csv(EVAL_FILE)\n",
    "df_data_val['review_tokens'] = df_data_val.review.apply(tokenizer.tokenize)\n",
    "df_data_val['dish_name_tokens'] = df_data_val.dish_name_tokens.apply(tokenizer.tokenize)\n",
    "df_data_val['review_labels'] = df_data_val.apply(lambda row: label_sent(row['dish_name_tokens'] row['review_tokens']), axis=1)\n",
    "\n",
    "\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 1\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "tr_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data_train['review_tokens']],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "tr_tags = pad_sequences(df_data_train['review_labels'],\n",
    "                     maxlen=MAX_LEN, padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "# create the mask to ignore the padded elements in the sequences.\n",
    "tr_masks = [[float(i>0) for i in ii] for ii in tr_inputs]\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "val_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data_val['review_tokens']],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "val_tags = pad_sequences(df_data_val['review_labels'],\n",
    "                     maxlen=MAX_LEN, padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "# create the mask to ignore the padded elements in the sequences.\n",
    "val_masks = [[float(i>0) for i in ii] for ii in val_inputs]\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Albert for Token Classification model, initialized with albert pretrained weights\n",
    "\n",
    "Create optimizers, FULL_FINETUNING for fine tuning with updation to albert pretrained weights as well, otherwise on updation to finetuning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenclassification = AlbertForTokenClassification(model, config)\n",
    "from torch.optim import Adam\n",
    "LEARNING_RATE = 0.000001\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model_tokenclassification.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model_tokenclassification.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training and Evaluation\n",
    "\n",
    "For every epoch, the results is saved in outputs folder, and it could resume from last saved epoch if training stopped and restarted.\n",
    "\n",
    "The below training supports for Multi GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os.path\n",
    "import torch.nn as nn\n",
    "EPOCH = 5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "ALBERT_FINETUNE_CHECKPOINT = \"outputs/finetune_checkpoint_5epoch_50neg_1e-5lr\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_tokenclassification = nn.DataParallel(model_tokenclassification)\n",
    "model_tokenclassification.to(device)\n",
    "if os.path.isfile(ALBERT_FINETUNE_CHECKPOINT):\n",
    "    print(f\"--- Load from checkpoint ---\")\n",
    "    checkpoint = torch.load(ALBERT_FINETUNE_CHECKPOINT)\n",
    "    model_tokenclassification.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    train_acc = checkpoint['train_acc']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_acc = checkpoint['val_acc']\n",
    "    \n",
    "else:\n",
    "    epoch = -1\n",
    "    train_losses, train_acc, val_losses, val_acc = [], [], [], []\n",
    "print(f\"--- Resume/Start training ---\")    \n",
    "for i in range(epoch+1, EPOCH): \n",
    "    print(f\"--- epoch: {i} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TRAIN loop\n",
    "    model_tokenclassification.train()\n",
    "    tr_loss, tr_acc, nb_tr_steps = 0, 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        b_outputs = model_tokenclassification(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        # Only keep active parts of the loss\n",
    "        b_active_loss = b_input_mask.view(-1) == 1\n",
    "        b_active_logits = b_outputs.view(-1, config.num_labels)[b_active_loss]\n",
    "        b_active_labels = b_labels.view(-1)[b_active_loss]\n",
    "        loss = loss_fct(b_active_logits, b_active_labels)\n",
    "        acc = torch.mean((torch.max(b_active_logits.detach(),1)[1] == b_active_labels.detach()).float())\n",
    "      \n",
    "        train_losses.append(loss.detach().item())\n",
    "        train_acc.append(acc)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        tr_acc += acc\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "    # print train loss per epoch\n",
    "    print(f\"Train loss: {(tr_loss/nb_tr_steps)}\")\n",
    "    print(f\"Train Accuracy: {(tr_acc/nb_tr_steps)}\")\n",
    "    print(f\"Train Time: {(time.time()-start_time)/60} mins\")\n",
    "\n",
    "    # VALIDATION on validation set\n",
    "    start_time = time.time()\n",
    "    model_tokenclassification.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            b_outputs = model_tokenclassification(b_input_ids, token_type_ids=None,\n",
    "                         attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            b_active_loss = b_input_mask.view(-1) == 1\n",
    "            b_active_logits = b_outputs.view(-1, config.num_labels)[b_active_loss]\n",
    "            b_active_labels = b_labels.view(-1)[b_active_loss]\n",
    "            loss = loss_fct(b_active_logits, b_active_labels)\n",
    "            acc = np.mean(np.argmax(b_active_logits.detach().cpu().numpy(), axis=1).flatten() == b_active_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "        eval_loss += loss.mean().item()\n",
    "        eval_acc += acc\n",
    "        nb_eval_steps += 1    \n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    eval_acc = eval_acc/nb_eval_steps\n",
    "    val_losses.append(eval_loss)\n",
    "    val_acc.append(eval_acc)\n",
    "    print(f\"Validation loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {(eval_acc)}\")\n",
    "    print(f\"Validation Time: {(time.time()-start_time)/60} mins\")    \n",
    "    \n",
    "    \n",
    "    print(f\"--- Save to checkpoint ---\")  \n",
    "    torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model_tokenclassification.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'train_losses': train_losses,\n",
    "        'train_acc': train_acc,\n",
    "        'val_losses': val_losses,\n",
    "        'val_acc': val_acc}\n",
    "        , ALBERT_FINETUNE_CHECKPOINT)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts):\n",
    "    tokenized_texts = [tokenizer.tokenize(txt) for txt in texts]\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_mask = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    datasampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=datasampler, batch_size=BATCH_SIZE) \n",
    "    \n",
    "    predicted_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model_tokenclassification(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "\n",
    "            predicted_labels.append(np.multiply(np.argmax(logits.detach().cpu().numpy(),axis=2), b_input_mask.detach().cpu().numpy()))\n",
    "    # np.concatenate(predicted_labels), to flatten list of arrays of batch_size * max_len into list of arrays of max_len\n",
    "    return np.concatenate(predicted_labels).astype(int), tokenized_texts\n",
    "\n",
    "texts = df_data_val.review.values\n",
    "predicted_labels, _ = predict(texts)\n",
    "df_data_val['predicted_review_label'] = list(predicted_labels)\n",
    "\n",
    "def get_dish_candidate_names(predicted_label, tokenized_text):\n",
    "    name_lists = []\n",
    "    if len(np.where(predicted_label>0)[0])>0:\n",
    "        name_idx_combined = np.where(predicted_label>0)[0]\n",
    "        name_idxs = np.split(name_idx_combined, np.where(np.diff(name_idx_combined) != 1)[0]+1)\n",
    "        name_lists.append([\" \".join(np.take(tokenized_text,name_idx)) for name_idx in name_idxs])\n",
    "        # If there duplicate names in the name_lists\n",
    "        name_lists = np.unique(name_lists)\n",
    "        return name_lists\n",
    "    else:\n",
    "        return None\n",
    "df_data_val['candidate_name']=df_data_val.apply(lambda row: get_dish_candidate_names(row.predicted_review_label, row.review_tokens)\n",
    "                                                , axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
