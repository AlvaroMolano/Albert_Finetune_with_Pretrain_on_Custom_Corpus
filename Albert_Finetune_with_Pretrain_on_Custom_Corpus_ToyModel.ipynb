{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Albert_Finetune_with_Pretrain_on_Custom_Corpus_ToyModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ8D7KScWtWtaP/Gu4LX4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/Albert_Finetune_with_Pretrain_on_Custom_Corpus_ToyModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQTY--EMFFH1",
        "colab_type": "text"
      },
      "source": [
        "Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNbI623UAtrO",
        "colab_type": "code",
        "outputId": "bf657c5b-70ae-4dca-9c66-beb35c6d77f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/dish_name_train.csv\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/dish_name_val.csv\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/restaurant_review.txt\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/restaurant_review_nopunct.txt\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/models_toy/albert_config.json\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/model_checkpoint/finetune_checkpoint\n",
        "!wget https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/model_checkpoint/pretrain_checkpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-07 09:32:50--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/dish_name_train.csv\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/dish_name_train.csv [following]\n",
            "--2020-03-07 09:32:51--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/dish_name_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123 [text/plain]\n",
            "Saving to: ‘dish_name_train.csv.1’\n",
            "\n",
            "\rdish_name_train.csv   0%[                    ]       0  --.-KB/s               \rdish_name_train.csv 100%[===================>]     123  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-07 09:32:51 (22.4 MB/s) - ‘dish_name_train.csv.1’ saved [123/123]\n",
            "\n",
            "--2020-03-07 09:32:52--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/dish_name_val.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/dish_name_val.csv [following]\n",
            "--2020-03-07 09:32:53--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/dish_name_val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123 [text/plain]\n",
            "Saving to: ‘dish_name_val.csv.1’\n",
            "\n",
            "dish_name_val.csv.1 100%[===================>]     123  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-07 09:32:53 (30.7 MB/s) - ‘dish_name_val.csv.1’ saved [123/123]\n",
            "\n",
            "--2020-03-07 09:32:54--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/restaurant_review.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/restaurant_review.txt [following]\n",
            "--2020-03-07 09:32:55--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/restaurant_review.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70 [text/plain]\n",
            "Saving to: ‘restaurant_review.txt.1’\n",
            "\n",
            "restaurant_review.t 100%[===================>]      70  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-07 09:32:55 (10.8 MB/s) - ‘restaurant_review.txt.1’ saved [70/70]\n",
            "\n",
            "--2020-03-07 09:32:56--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/data_toy/restaurant_review_nopunct.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/restaurant_review_nopunct.txt [following]\n",
            "--2020-03-07 09:32:56--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/data_toy/restaurant_review_nopunct.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68 [text/plain]\n",
            "Saving to: ‘restaurant_review_nopunct.txt.1’\n",
            "\n",
            "restaurant_review_n 100%[===================>]      68  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-07 09:32:56 (12.7 MB/s) - ‘restaurant_review_nopunct.txt.1’ saved [68/68]\n",
            "\n",
            "--2020-03-07 09:32:58--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/models_toy/albert_config.json\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/models_toy/albert_config.json [following]\n",
            "--2020-03-07 09:32:58--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/models_toy/albert_config.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 478 [text/plain]\n",
            "Saving to: ‘albert_config.json.1’\n",
            "\n",
            "albert_config.json. 100%[===================>]     478  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-07 09:32:58 (64.7 MB/s) - ‘albert_config.json.1’ saved [478/478]\n",
            "\n",
            "--2020-03-07 09:33:00--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/model_checkpoint/finetune_checkpoint\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/model_checkpoint/finetune_checkpoint [following]\n",
            "--2020-03-07 09:33:00--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/model_checkpoint/finetune_checkpoint\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94415192 (90M) [application/octet-stream]\n",
            "Saving to: ‘finetune_checkpoint.1’\n",
            "\n",
            "finetune_checkpoint 100%[===================>]  90.04M  97.0MB/s    in 0.9s    \n",
            "\n",
            "2020-03-07 09:33:02 (97.0 MB/s) - ‘finetune_checkpoint.1’ saved [94415192/94415192]\n",
            "\n",
            "--2020-03-07 09:33:03--  https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/raw/master/model_checkpoint/pretrain_checkpoint\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/model_checkpoint/pretrain_checkpoint [following]\n",
            "--2020-03-07 09:33:04--  https://raw.githubusercontent.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/master/model_checkpoint/pretrain_checkpoint\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95601189 (91M) [application/octet-stream]\n",
            "Saving to: ‘pretrain_checkpoint.1’\n",
            "\n",
            "pretrain_checkpoint 100%[===================>]  91.17M   153MB/s    in 0.6s    \n",
            "\n",
            "2020-03-07 09:33:05 (153 MB/s) - ‘pretrain_checkpoint.1’ saved [95601189/95601189]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf7OlIbVHVK9",
        "colab_type": "text"
      },
      "source": [
        "Step 1 Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXCLEuXGHT4Z",
        "colab_type": "code",
        "outputId": "163f52e5-e8b3-433d-ccf1-9752ebf7a4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "!git clone https://github.com/kwonmha/bert-vocab-builder.git\n",
        "!python ./bert-vocab-builder/subword_builder.py --corpus_filepattern \"restaurant_review_nopunct.txt\" --output_filename \"vocab.txt\" --min_count 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-vocab-builder'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 88\u001b[K\n",
            "Unpacking objects:   1% (1/91)   \rUnpacking objects:   2% (2/91)   \rUnpacking objects:   3% (3/91)   \rUnpacking objects:   4% (4/91)   \rUnpacking objects:   5% (5/91)   \rUnpacking objects:   6% (6/91)   \rUnpacking objects:   7% (7/91)   \rUnpacking objects:   8% (8/91)   \rUnpacking objects:   9% (9/91)   \rUnpacking objects:  10% (10/91)   \rUnpacking objects:  12% (11/91)   \rUnpacking objects:  13% (12/91)   \rUnpacking objects:  14% (13/91)   \rUnpacking objects:  15% (14/91)   \rUnpacking objects:  16% (15/91)   \rUnpacking objects:  17% (16/91)   \rUnpacking objects:  18% (17/91)   \rUnpacking objects:  19% (18/91)   \rUnpacking objects:  20% (19/91)   \rUnpacking objects:  21% (20/91)   \rUnpacking objects:  23% (21/91)   \rUnpacking objects:  24% (22/91)   \rUnpacking objects:  25% (23/91)   \rUnpacking objects:  26% (24/91)   \rUnpacking objects:  27% (25/91)   \rUnpacking objects:  28% (26/91)   \rUnpacking objects:  29% (27/91)   \rUnpacking objects:  30% (28/91)   \rUnpacking objects:  31% (29/91)   \rUnpacking objects:  32% (30/91)   \rUnpacking objects:  34% (31/91)   \rUnpacking objects:  35% (32/91)   \rUnpacking objects:  36% (33/91)   \rUnpacking objects:  37% (34/91)   \rUnpacking objects:  38% (35/91)   \rUnpacking objects:  39% (36/91)   \rUnpacking objects:  40% (37/91)   \rUnpacking objects:  41% (38/91)   \rUnpacking objects:  42% (39/91)   \rUnpacking objects:  43% (40/91)   \rUnpacking objects:  45% (41/91)   \rUnpacking objects:  46% (42/91)   \rUnpacking objects:  47% (43/91)   \rUnpacking objects:  48% (44/91)   \rUnpacking objects:  49% (45/91)   \rUnpacking objects:  50% (46/91)   \rUnpacking objects:  51% (47/91)   \rUnpacking objects:  52% (48/91)   \rUnpacking objects:  53% (49/91)   \rUnpacking objects:  54% (50/91)   \rUnpacking objects:  56% (51/91)   \rUnpacking objects:  57% (52/91)   \rUnpacking objects:  58% (53/91)   \rUnpacking objects:  59% (54/91)   \rUnpacking objects:  60% (55/91)   \rUnpacking objects:  61% (56/91)   \rUnpacking objects:  62% (57/91)   \rUnpacking objects:  63% (58/91)   \rUnpacking objects:  64% (59/91)   \rUnpacking objects:  65% (60/91)   \rUnpacking objects:  67% (61/91)   \rUnpacking objects:  68% (62/91)   \rUnpacking objects:  69% (63/91)   \rUnpacking objects:  70% (64/91)   \rUnpacking objects:  71% (65/91)   \rUnpacking objects:  72% (66/91)   \rUnpacking objects:  73% (67/91)   \rUnpacking objects:  74% (68/91)   \rUnpacking objects:  75% (69/91)   \rUnpacking objects:  76% (70/91)   \rUnpacking objects:  78% (71/91)   \rUnpacking objects:  79% (72/91)   \rUnpacking objects:  80% (73/91)   \rUnpacking objects:  81% (74/91)   \rUnpacking objects:  82% (75/91)   \rUnpacking objects:  83% (76/91)   \rUnpacking objects:  84% (77/91)   \rUnpacking objects:  85% (78/91)   \rUnpacking objects:  86% (79/91)   \rUnpacking objects:  87% (80/91)   \rUnpacking objects:  89% (81/91)   \rUnpacking objects:  90% (82/91)   \rUnpacking objects:  91% (83/91)   \rUnpacking objects:  92% (84/91)   \rUnpacking objects:  93% (85/91)   \rUnpacking objects:  94% (86/91)   \rUnpacking objects:  95% (87/91)   \rUnpacking objects:  96% (88/91)   \rUnpacking objects:  97% (89/91)   \rUnpacking objects:  98% (90/91)   \rUnpacking objects: 100% (91/91)   \rUnpacking objects: 100% (91/91), done.\n",
            "WARNING:tensorflow:From ./bert-vocab-builder/subword_builder.py:81: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/bert-vocab-builder/tokenizer.py:133: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0307 09:30:46.036749 140355133376384 module_wrapper.py:139] From /content/bert-vocab-builder/tokenizer.py:133: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "['./restaurant_review_nopunct.txt']\n",
            "WARNING:tensorflow:From /content/bert-vocab-builder/tokenizer.py:138: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0307 09:30:46.038344 140355133376384 module_wrapper.py:139] From /content/bert-vocab-builder/tokenizer.py:138: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "0.0008256435394287109 for reading read file : ./restaurant_review_nopunct.txt\n",
            "read all files\n",
            "WARNING:tensorflow:From /content/bert-vocab-builder/text_encoder.py:588: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0307 09:30:46.045308 140355133376384 module_wrapper.py:139] From /content/bert-vocab-builder/text_encoder.py:588: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Iteration 0\n",
            "I0307 09:30:46.045545 140355133376384 text_encoder.py:588] Iteration 0\n",
            "INFO:tensorflow:vocab_size = 125\n",
            "I0307 09:30:46.055774 140355133376384 text_encoder.py:660] vocab_size = 125\n",
            "INFO:tensorflow:Iteration 1\n",
            "I0307 09:30:46.055977 140355133376384 text_encoder.py:588] Iteration 1\n",
            "INFO:tensorflow:vocab_size = 89\n",
            "I0307 09:30:46.060989 140355133376384 text_encoder.py:660] vocab_size = 89\n",
            "INFO:tensorflow:Iteration 2\n",
            "I0307 09:30:46.061177 140355133376384 text_encoder.py:588] Iteration 2\n",
            "INFO:tensorflow:vocab_size = 89\n",
            "I0307 09:30:46.065740 140355133376384 text_encoder.py:660] vocab_size = 89\n",
            "INFO:tensorflow:Iteration 3\n",
            "I0307 09:30:46.065906 140355133376384 text_encoder.py:588] Iteration 3\n",
            "INFO:tensorflow:vocab_size = 89\n",
            "I0307 09:30:46.070376 140355133376384 text_encoder.py:660] vocab_size = 89\n",
            "INFO:tensorflow:Iteration 4\n",
            "I0307 09:30:46.070602 140355133376384 text_encoder.py:588] Iteration 4\n",
            "INFO:tensorflow:vocab_size = 89\n",
            "I0307 09:30:46.075330 140355133376384 text_encoder.py:660] vocab_size = 89\n",
            "total vocab size : 159, 0.036550045013427734 seconds elapsed \n",
            "INFO:tensorflow:vocab_size = 159\n",
            "I0307 09:30:46.080471 140355133376384 text_encoder.py:686] vocab_size = 159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhkSh3BFOLJo",
        "colab_type": "text"
      },
      "source": [
        "Step 2 Create Pretrain files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji3h4gKsMujw",
        "colab_type": "code",
        "outputId": "6a62d49f-c6c9-4025-b84f-ca00a0d424dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/ALBERT\n",
        "!python ./ALBERT/create_pretraining_data.py --input_file \"restaurant_review.txt\" --output_file \"restaurant_review_train\" --vocab_file \"vocab.txt\" --max_seq_length=64"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Cloning into 'ALBERT'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Total 281 (delta 0), reused 0 (delta 0), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (281/281), 193.91 KiB | 6.06 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "INFO:tensorflow:*** Reading from input files ***\n",
            "I0307 09:30:59.820739 140260795062144 create_pretraining_data.py:631] *** Reading from input files ***\n",
            "INFO:tensorflow:  ./restaurant_review.txt\n",
            "I0307 09:30:59.820976 140260795062144 create_pretraining_data.py:633]   ./restaurant_review.txt\n",
            "INFO:tensorflow:number of instances: 40\n",
            "I0307 09:30:59.845409 140260795062144 create_pretraining_data.py:641] number of instances: 40\n",
            "INFO:tensorflow:*** Writing to output files ***\n",
            "I0307 09:30:59.846092 140260795062144 create_pretraining_data.py:644] *** Writing to output files ***\n",
            "INFO:tensorflow:  restaurant_review_train\n",
            "I0307 09:30:59.846192 140260795062144 create_pretraining_data.py:646]   restaurant_review_train\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.846916 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the mala steamboat a l ##o ##t [MASK] [SEP] [MASK] [MASK] rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e ! [SEP]\n",
            "I0307 09:30:59.847192 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the mala steamboat a l ##o ##t [MASK] [SEP] [MASK] [MASK] rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e ! [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 10 8 14 93 25 22 5 4 5 5 9 139 25 32 23 26 125 5 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847353 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 10 8 14 93 25 22 5 4 5 5 9 139 25 32 23 26 125 5 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847483 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847625 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847740 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 10 12 13 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847859 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 10 12 13 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 121 6 13 146 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.847957 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 121 6 13 146 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.848063 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:30:59.848152 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.848601 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the mala steamboat [MASK] l ##o ##t . [SEP] [MASK] [MASK] rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP]\n",
            "I0307 09:30:59.848851 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the mala steamboat [MASK] l ##o ##t . [SEP] [MASK] [MASK] rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 10 8 5 93 25 22 121 4 5 5 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849159 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 10 8 5 93 25 22 121 4 5 5 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849361 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849501 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849622 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 5 6 12 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849716 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 5 6 12 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 8 14 6 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.849812 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 8 14 6 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.849920 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:30:59.850008 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.850342 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] [MASK] the [MASK] steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.850466 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] [MASK] the [MASK] steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 5 6 5 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.850602 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 5 6 5 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.850722 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.850832 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.850972 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 16 18 19 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.851066 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 16 18 19 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 142 150 11 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.851155 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 142 150 11 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.851248 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.851330 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.851669 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the mala [MASK] [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n [MASK] t taste n ##i ##c ##e [MASK] [SEP]\n",
            "I0307 09:30:59.851791 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the mala [MASK] [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n [MASK] t taste n ##i ##c ##e [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 10 5 5 93 25 22 121 4 6 13 9 139 25 32 23 26 5 146 7 89 30 34 32 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.851912 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 10 5 5 93 25 22 121 4 6 13 9 139 25 32 23 26 5 146 7 89 30 34 32 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.852025 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.852135 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.852246 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 5 6 20 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.852340 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 5 6 20 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 8 14 125 142 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.852430 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 8 14 125 142 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.852536 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:30:59.852623 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.852941 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken [MASK] d ##o ##e ##s ##n ' t l n ##i ##c ##e ! [SEP] [MASK] like the mala [MASK] a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.853062 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken [MASK] d ##o ##e ##s ##n ' t l n ##i ##c ##e ! [SEP] [MASK] like the mala [MASK] a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 5 139 25 32 23 26 125 146 93 89 30 34 32 142 4 5 11 6 10 5 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853177 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 5 139 25 32 23 26 125 146 93 89 30 34 32 142 4 5 11 6 10 5 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853290 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853402 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853530 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 11 18 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853623 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 3 11 18 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 9 7 150 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.853717 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 9 7 150 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.853815 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.853908 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.854218 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP] i [MASK] the mala steamboat a [MASK] [MASK] [MASK] . [SEP]\n",
            "I0307 09:30:59.854334 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP] i [MASK] the mala steamboat a [MASK] [MASK] [MASK] . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 150 5 6 10 8 14 5 5 5 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.854442 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 150 5 6 10 8 14 5 5 5 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.854570 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.854687 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.854799 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 19 24 25 26 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.940319 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 19 24 25 26 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 11 93 25 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.940603 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 11 93 25 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.940753 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.940883 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.941552 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] chicken rice d ##o ##e ##s ##n ' [MASK] [MASK] n ##i ##c ##e ! [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.941748 140260795062144 create_pretraining_data.py:190] tokens: [CLS] [MASK] chicken rice d ##o ##e ##s ##n ' [MASK] [MASK] n ##i ##c ##e ! [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 5 13 9 139 25 32 23 26 125 5 5 89 30 34 32 142 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.941961 140260795062144 create_pretraining_data.py:200] input_ids: 3 5 13 9 139 25 32 23 26 125 5 5 89 30 34 32 142 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.942119 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.942280 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.942436 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 10 11 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.942585 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 1 10 11 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 6 146 7 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.942712 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 6 146 7 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.942848 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.942968 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.943509 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] [MASK] rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP] i like the mala [MASK] a l ##o ##t [MASK] [SEP]\n",
            "I0307 09:30:59.943658 140260795062144 create_pretraining_data.py:190] tokens: [CLS] [MASK] [MASK] rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP] i like the mala [MASK] a l ##o ##t [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 3 5 5 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 150 11 6 10 5 14 93 25 22 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.943785 140260795062144 create_pretraining_data.py:200] input_ids: 3 5 5 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 150 11 6 10 5 14 93 25 22 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.943909 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.944023 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.944135 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 2 22 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.944229 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 1 2 22 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 6 13 8 121 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.944320 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 6 13 8 121 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.944414 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.944525 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.944877 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the mala [MASK] [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e ! [SEP]\n",
            "I0307 09:30:59.945006 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the mala [MASK] [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e ! [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 10 5 5 93 25 22 121 4 6 13 9 139 25 32 23 26 125 5 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945124 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 10 5 5 93 25 22 121 4 6 13 9 139 25 32 23 26 125 5 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945236 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945346 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945461 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 5 6 14 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945567 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 5 6 14 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 8 14 9 146 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.945658 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 8 14 9 146 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.945758 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:30:59.945847 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.946165 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken [MASK] d ##o ##e ##s ##n ' [MASK] [MASK] n ##i ##c ##e ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.946281 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken [MASK] d ##o ##e ##s ##n ' [MASK] [MASK] n ##i ##c ##e ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 5 139 25 32 23 26 125 5 5 89 30 34 32 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946393 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 5 139 25 32 23 26 125 5 5 89 30 34 32 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946518 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946630 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946738 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 10 11 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946838 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 3 10 11 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 9 146 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.946932 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 9 146 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.947038 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.947127 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.947439 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] [MASK] the mala [MASK] a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.947571 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] [MASK] the mala [MASK] a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 5 6 10 5 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.947684 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 5 6 10 5 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.947797 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.947913 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.948024 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 16 18 19 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.948116 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 16 18 19 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 142 150 11 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.948204 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 142 150 11 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.948296 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.948384 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.948726 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] ##' rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.948852 140260795062144 create_pretraining_data.py:190] tokens: [CLS] [MASK] ##' rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e [MASK] [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 5 81 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.948965 140260795062144 create_pretraining_data.py:200] input_ids: 3 5 81 9 139 25 32 23 26 125 146 7 89 30 34 32 5 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.949075 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.949185 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.949296 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 2 16 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.949389 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 1 2 16 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 6 13 142 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.949478 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 6 13 142 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.949590 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.949679 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:30:59.949994 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] : chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e [MASK] [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:30:59.950112 140260795062144 create_pretraining_data.py:190] tokens: [CLS] : chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e [MASK] [SEP] [MASK] like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 157 13 9 139 25 32 23 26 125 5 7 89 30 34 32 5 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950223 140260795062144 create_pretraining_data.py:200] input_ids: 3 157 13 9 139 25 32 23 26 125 5 7 89 30 34 32 5 4 5 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950334 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950443 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950566 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 10 16 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950660 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 1 10 16 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 6 146 142 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:30:59.950750 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 6 146 142 150 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:30:59.950848 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:30:59.950934 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.050506 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste [MASK] [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:31:00.050708 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste [MASK] [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 5 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.050818 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 5 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.050900 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.050975 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.051045 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.051106 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.051166 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.051232 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:31:00.051289 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.051647 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:31:00.051768 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste n [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.051860 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 89 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.051935 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.052045 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.052161 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.052258 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.052352 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.052460 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:31:00.052586 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.052894 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' [MASK] ##% n ##i ##c ##e ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:31:00.052982 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' [MASK] ##% n ##i ##c ##e ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 5 83 89 30 34 32 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053061 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 5 83 89 30 34 32 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053133 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053203 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053271 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 10 11 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053329 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 3 10 11 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 9 146 7 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053387 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 9 146 7 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.053448 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:31:00.053539 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.053834 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the mala steamboat a ##{ [MASK] [MASK] . [SEP] [MASK] chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP]\n",
            "I0307 09:31:00.053919 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the mala steamboat a ##{ [MASK] [MASK] . [SEP] [MASK] chicken rice d ##o ##e ##s ##n ' t taste n ##i ##c ##e ! [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 10 8 14 20 5 5 121 4 5 13 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.053996 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 10 8 14 20 5 5 121 4 5 13 9 139 25 32 23 26 125 146 7 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.054095 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.054211 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.054343 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 7 8 9 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.054447 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 7 8 9 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 93 25 22 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.054579 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 93 25 22 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.054679 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:31:00.054743 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.055028 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] i like the [MASK] steamboat [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e [MASK] [SEP]\n",
            "I0307 09:31:00.055115 140260795062144 create_pretraining_data.py:190] tokens: [CLS] i like the [MASK] steamboat [MASK] l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' [MASK] taste n ##i ##c ##e [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 3 150 11 6 5 8 5 93 25 22 121 4 6 13 9 139 25 32 23 26 125 5 7 89 30 34 32 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055192 140260795062144 create_pretraining_data.py:200] input_ids: 3 150 11 6 5 8 5 93 25 22 121 4 6 13 9 139 25 32 23 26 125 5 7 89 30 34 32 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055268 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055383 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055533 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 4 6 21 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055636 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 4 6 21 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 10 14 146 142 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.055738 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 10 14 146 142 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.055835 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:31:00.055898 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.056170 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] [MASK] the mala steamboat a l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' t [MASK] n ##i ##c ##e ! [SEP]\n",
            "I0307 09:31:00.056252 140260795062144 create_pretraining_data.py:190] tokens: [CLS] [MASK] [MASK] the mala steamboat a l ##o ##t . [SEP] the chicken rice d ##o ##e ##s ##n ' t [MASK] n ##i ##c ##e ! [SEP]\n",
            "INFO:tensorflow:input_ids: 3 5 5 6 10 8 14 93 25 22 121 4 6 13 9 139 25 32 23 26 125 146 5 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056329 140260795062144 create_pretraining_data.py:200] input_ids: 3 5 5 6 10 8 14 93 25 22 121 4 6 13 9 139 25 32 23 26 125 146 5 89 30 34 32 142 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056402 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056472 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056585 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 1 2 13 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056648 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 1 2 13 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 150 11 13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.056725 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 150 11 13 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.149557 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0307 09:31:00.149679 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0307 09:31:00.150082 140260795062144 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste [MASK] [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "I0307 09:31:00.150194 140260795062144 create_pretraining_data.py:190] tokens: [CLS] the chicken rice d ##o ##e ##s ##n ' t taste [MASK] [MASK] [MASK] [MASK] ! [SEP] i like the mala steamboat a l ##o ##t . [SEP]\n",
            "INFO:tensorflow:input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 5 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150292 140260795062144 create_pretraining_data.py:200] input_ids: 3 6 13 9 139 25 32 23 26 125 146 7 5 5 5 5 142 4 150 11 6 10 8 14 93 25 22 121 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150369 140260795062144 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150440 140260795062144 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150532 140260795062144 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150595 140260795062144 create_pretraining_data.py:200] masked_lm_positions: 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0307 09:31:00.150655 140260795062144 create_pretraining_data.py:200] masked_lm_ids: 89 30 34 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "I0307 09:31:00.150717 140260795062144 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0307 09:31:00.150773 140260795062144 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:Wrote 40 total instances\n",
            "I0307 09:31:00.155220 140260795062144 create_pretraining_data.py:205] Wrote 40 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkZ09sPUPKjG",
        "colab_type": "text"
      },
      "source": [
        "Step 3 Pretrain Albert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrmaAC3tL_-x",
        "colab_type": "code",
        "outputId": "7233dd28-c00d-4b57-ec88-04aa263afb05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tfrecord"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 9.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 20.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=93fb317ad3703af018fa470076aebb1a56ea1e7783b51a0fa5685e6abdf35fd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 tokenizers-0.5.2 transformers-2.5.1\n",
            "Collecting tfrecord\n",
            "  Downloading https://files.pythonhosted.org/packages/37/39/a871605efc23f13e0f7979445225897cf2e2dcbf031b9fb9e16c04740f11/tfrecord-1.9.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfrecord) (1.17.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tfrecord) (3.10.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->tfrecord) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tfrecord) (45.2.0)\n",
            "Building wheels for collected packages: tfrecord\n",
            "  Building wheel for tfrecord (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tfrecord: filename=tfrecord-1.9-cp36-none-any.whl size=8406 sha256=0505fbd8810abb34b89f89b18d39357bd8928a60ce6e9da2209343b3b1082a4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/f3/5b/de653b822385afe03117ed7e3759bb110e6c6a20bac753392a\n",
            "Successfully built tfrecord\n",
            "Installing collected packages: tfrecord\n",
            "Successfully installed tfrecord-1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urw76yyqaS2v",
        "colab_type": "text"
      },
      "source": [
        "Albert Model for Pretraining: training task: masked LM + sequence order prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNAOT-X8ALIy",
        "colab_type": "code",
        "outputId": "57437e37-ebdf-40fc-8085-60592b3d5ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from transformers.modeling_albert import AlbertModel, AlbertPreTrainedModel\n",
        "from transformers.configuration_albert import AlbertConfig\n",
        "import torch.nn as nn\n",
        "class AlbertSequenceOrderHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        self.bias = nn.Parameter(torch.zeros(2))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        prediction_scores = hidden_states + self.bias\n",
        "\n",
        "        return prediction_scores\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.modeling_bert import ACT2FN\n",
        "class AlbertForPretrain(AlbertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.albert = AlbertModel(config)       \n",
        "        \n",
        "        # For Masked LM\n",
        "        # The original huggingface implementation, created new output weights via dense layer\n",
        "        # However the original Albert \n",
        "        self.predictions_dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
        "        self.predictions_activation = ACT2FN[config.hidden_act]\n",
        "        self.predictions_LayerNorm = nn.LayerNorm(config.embedding_size)\n",
        "        self.predictions_bias = nn.Parameter(torch.zeros(config.vocab_size)) \n",
        "        self.predictions_decoder = nn.Linear(config.embedding_size, config.vocab_size)\n",
        "        \n",
        "        self.predictions_decoder.weight = self.albert.embeddings.word_embeddings.weight\n",
        "        \n",
        "        # For sequence order prediction\n",
        "        self.seq_relationship = AlbertSequenceOrderHead(config)\n",
        "        \n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        masked_lm_labels=None,\n",
        "        seq_relationship_labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.albert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        \n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        \n",
        "        sequence_output = outputs[0]\n",
        "        \n",
        "        sequence_output = self.predictions_dense(sequence_output)\n",
        "        sequence_output = self.predictions_activation(sequence_output)\n",
        "        sequence_output = self.predictions_LayerNorm(sequence_output)\n",
        "        prediction_scores = self.predictions_decoder(sequence_output)\n",
        "\n",
        "        \n",
        "        if masked_lm_labels is not None:\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size)\n",
        "                                      , masked_lm_labels.view(-1))\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        seq_relationship_scores = self.seq_relationship(pooled_output)\n",
        "        if seq_relationship_labels is not None:  \n",
        "            seq_relationship_loss = loss_fct(seq_relationship_scores.view(-1, 2), seq_relationship_labels.view(-1))\n",
        "        \n",
        "        loss = masked_lm_loss + seq_relationship_loss\n",
        "        \n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf2GmxsgYero",
        "colab_type": "text"
      },
      "source": [
        "LAMB Optimizer, cite \"https://github.com/cybertronai/pytorch-lamb\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asH80VEQXYBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "class Lamb(Optimizer):\n",
        "    r\"\"\"Implements Lamb algorithm.\n",
        "    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        adam (bool, optional): always use trust ratio = 1, which turns this into\n",
        "            Adam. Useful for comparison purposes.\n",
        "    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:\n",
        "        https://arxiv.org/abs/1904.00962\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,\n",
        "                 weight_decay=0, adam=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay)\n",
        "        self.adam = adam\n",
        "        super(Lamb, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                # Paper v3 does not use debiasing.\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "                # Apply bias to lr to avoid broadcast.\n",
        "                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "\n",
        "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
        "                if group['weight_decay'] != 0:\n",
        "                    adam_step.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                adam_norm = adam_step.pow(2).sum().sqrt()\n",
        "                if weight_norm == 0 or adam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / adam_norm\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = adam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "                if self.adam:\n",
        "                    trust_ratio = 1\n",
        "\n",
        "                p.data.add_(-step_size * trust_ratio, adam_step)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8BGB45COhY",
        "colab_type": "code",
        "outputId": "a60a3a21-853e-48de-9970-8b77c2f7c08c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "import time\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from tfrecord.torch.dataset import TFRecordDataset\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 40\n",
        "BATCH_SIZE = 2\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "print(f\"--- Resume/Start training ---\")   \n",
        "feat_map = {\"input_ids\": \"int\", \n",
        "           \"input_mask\": \"int\",\n",
        "           \"segment_ids\": \"int\",\n",
        "           \"next_sentence_labels\": \"int\",\n",
        "           \"masked_lm_positions\": \"int\",\n",
        "           \"masked_lm_ids\": \"int\"}\n",
        "pretrain_file = 'restaurant_review_train'\n",
        "\n",
        "# Create albert pretrain model\n",
        "config = AlbertConfig.from_json_file(\"albert_config.json\")\n",
        "albert_pretrain = AlbertForPretrain(config)\n",
        "# Create optimizer\n",
        "optimizer = Lamb([{\"params\": [p for n, p in list(albert_pretrain.named_parameters())]}], lr=LEARNING_RATE)\n",
        "albert_pretrain.train()\n",
        "dataset = TFRecordDataset(pretrain_file, index_path = None, description=feat_map)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "tmp_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "if os.path.isfile('pretrain_checkpoint'):\n",
        "    print(f\"--- Load from checkpoint ---\")\n",
        "    checkpoint = torch.load(\"pretrain_checkpoint\")\n",
        "    albert_pretrain.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    losses = checkpoint['losses']\n",
        "    \n",
        "else:\n",
        "    epoch = -1\n",
        "    losses = []\n",
        "for e in range(epoch+1, EPOCH):\n",
        "    for batch in loader:\n",
        "        b_input_ids = batch['input_ids'].long() \n",
        "        b_token_type_ids = batch['segment_ids'].long() \n",
        "        b_seq_relationship_labels = batch['next_sentence_labels'].long()\n",
        "\n",
        "        # Convert the dataformat from loaded decoded format into format \n",
        "        # loaded format is created by google's Albert create_pretrain.py script\n",
        "        # required by huggingfaces pytorch implementation of albert\n",
        "        mask_rows = np.nonzero(batch['masked_lm_positions'].numpy())[0]\n",
        "        mask_cols = batch['masked_lm_positions'].numpy()[batch['masked_lm_positions'].numpy()!=0]\n",
        "        b_attention_mask = np.zeros((BATCH_SIZE,64),dtype=np.int64)\n",
        "        b_attention_mask[mask_rows,mask_cols] = 1\n",
        "        b_masked_lm_labels = np.zeros((BATCH_SIZE,64),dtype=np.int64) - 100\n",
        "        b_masked_lm_labels[mask_rows,mask_cols] = batch['masked_lm_ids'].numpy()[batch['masked_lm_positions'].numpy()!=0]     \n",
        "        b_attention_mask=torch.tensor(b_attention_mask).long()\n",
        "        b_masked_lm_labels=torch.tensor(b_masked_lm_labels).long()\n",
        "\n",
        "\n",
        "        loss = albert_pretrain(input_ids = b_input_ids\n",
        "                              , attention_mask = b_attention_mask\n",
        "                              , token_type_ids = b_token_type_ids\n",
        "                              , masked_lm_labels = b_masked_lm_labels \n",
        "                              , seq_relationship_labels = b_seq_relationship_labels)\n",
        "\n",
        "        # clears old gradients\n",
        "        optimizer.zero_grad()\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=albert_pretrain.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        tmp_loss += loss.detach().item()\n",
        "\n",
        "    # print metrics and save to checkpoint every epoch\n",
        "    print(f\"Epoch: {e}\")\n",
        "    print(f\"Train loss: {(tmp_loss/20)}\")\n",
        "    print(f\"Train Time: {(time.time()-start_time)/60} mins\")  \n",
        "    losses.append(tmp_loss/20)\n",
        "\n",
        "    tmp_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    torch.save({'model_state_dict': albert_pretrain.state_dict(),'optimizer_state_dict': optimizer.state_dict(),\n",
        "               'epoch': e, 'loss': loss,'losses': losses}\n",
        "           , 'pretrain_checkpoint')\n",
        "from matplotlib import pyplot as plot\n",
        "plot.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Resume/Start training ---\n",
            "--- Load from checkpoint ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3752ac9fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni1EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnAfV9xzdehf",
        "colab_type": "text"
      },
      "source": [
        "Step 4 Albert Finetuning for token classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuXteiH_CfhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# At the time of writing, Hugging face didnt provide the class object for \n",
        "# AlbertForTokenClassification, hence write your own defination below\n",
        "from transformers.modeling_albert import AlbertModel, AlbertPreTrainedModel\n",
        "from transformers.configuration_albert import AlbertConfig\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "class AlbertForTokenClassification(AlbertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, albert, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.albert = albert\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.albert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtUsFbgii-kr",
        "colab_type": "code",
        "outputId": "a7f7f5cf-ba77-4b2d-c831-459f42b18cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "def label_sent(name_tokens, sent_tokens):\n",
        "    label = []\n",
        "    i = 0\n",
        "    if len(name_tokens)>len(sent_tokens):\n",
        "        label = np.zeros(len(sent_tokens))\n",
        "    else:\n",
        "        while i<len(sent_tokens):\n",
        "            found_match = False\n",
        "            if name_tokens[0] == sent_tokens[i]:       \n",
        "                found_match = True\n",
        "                for j in range(len(name_tokens)-1):\n",
        "                    if ((i+j+1)>=len(sent_tokens)):\n",
        "                        return label\n",
        "                    if name_tokens[j+1] != sent_tokens[i+j+1]:\n",
        "                        found_match = False\n",
        "                if found_match:\n",
        "                    label.extend(list(np.ones(len(name_tokens)).astype(int)))\n",
        "                    i = i + len(name_tokens)\n",
        "                else: \n",
        "                    label.extend([0])\n",
        "                    i = i+ 1\n",
        "            else:\n",
        "                label.extend([0])\n",
        "                i=i+1\n",
        "    return label\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "tokenizer = BertTokenizer(vocab_file=\"vocab.txt\")\n",
        "\n",
        "df_data_train = pd.read_csv(\"dish_name_train.csv\")\n",
        "df_data_train['name_tokens'] = df_data_train['dish_name'].apply(tokenizer.tokenize)\n",
        "df_data_train['review_tokens'] = df_data_train.review.apply(tokenizer.tokenize)\n",
        "df_data_train['review_label'] = df_data_train.apply(lambda row: label_sent(row['name_tokens'], row['review_tokens']), axis=1)\n",
        "\n",
        "df_data_val = pd.read_csv(\"dish_name_val.csv\")\n",
        "df_data_val = df_data_val.dropna().reset_index()\n",
        "df_data_val['name_tokens'] = df_data_val['dish_name'].apply(tokenizer.tokenize)\n",
        "df_data_val['review_tokens'] = df_data_val.review.apply(tokenizer.tokenize)\n",
        "df_data_val['review_label'] = df_data_val.apply(lambda row: label_sent(row['name_tokens'], row['review_tokens']), axis=1)\n",
        "\n",
        "MAX_LEN = 64\n",
        "BATCH_SIZE = 1\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "tr_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data_train['review_tokens']],maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "tr_tags = pad_sequences(df_data_train['review_label'],maxlen=MAX_LEN, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
        "# create the mask to ignore the padded elements in the sequences.\n",
        "tr_masks = [[float(i>0) for i in ii] for ii in tr_inputs]\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "val_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data_val['review_tokens']],maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "val_tags = pad_sequences(df_data_val['review_label'],maxlen=MAX_LEN, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
        "# create the mask to ignore the padded elements in the sequences.\n",
        "val_masks = [[float(i>0) for i in ii] for ii in val_inputs]\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om08SlrYjCc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_tokenclassification = AlbertForTokenClassification(albert_pretrain.albert, config)\n",
        "from torch.optim import Adam\n",
        "LEARNING_RATE = 0.0000003\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model_tokenclassification.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model_tokenclassification.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCZqnDNhlA_X",
        "colab_type": "code",
        "outputId": "42e1f083-a538-4c48-a8f9-3df3dd3c07f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import os.path\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "EPOCH = 800\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "start_time = time.time()\n",
        "tr_loss, tr_acc, nb_tr_steps = 0, 0, 0\n",
        "eval_loss, eval_acc, nb_eval_steps = 0, 0, 0\n",
        "\n",
        "if os.path.isfile('finetune_checkpoint'):\n",
        "    print(f\"--- Load from checkpoint ---\")\n",
        "    checkpoint = torch.load(\"finetune_checkpoint\")\n",
        "    model_tokenclassification.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    train_accs = checkpoint['train_accs']\n",
        "    eval_losses = checkpoint['eval_losses']\n",
        "    eval_accs = checkpoint['eval_accs']\n",
        "    \n",
        "else:\n",
        "    epoch = -1\n",
        "    train_losses,train_accs,eval_losses,eval_accs = [],[],[],[]\n",
        "\n",
        "print(f\"--- Resume/Start training ---\")    \n",
        "for e in range(epoch+1, EPOCH): \n",
        "    \n",
        "    # TRAIN loop\n",
        "    model_tokenclassification.train()\n",
        "    \n",
        "    for batch in train_dataloader:\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        b_outputs = model_tokenclassification(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        \n",
        "        ce_loss_fct = CrossEntropyLoss()\n",
        "        # Only keep active parts of the loss\n",
        "        b_active_loss = b_input_mask.view(-1) == 1\n",
        "        b_active_logits = b_outputs.view(-1, config.num_labels)[b_active_loss]\n",
        "        b_active_labels = b_labels.view(-1)[b_active_loss]\n",
        "        \n",
        "        loss = ce_loss_fct(b_active_logits, b_active_labels)\n",
        "        acc = torch.mean((torch.max(b_active_logits.detach(),1)[1] == b_active_labels.detach()).float())\n",
        "\n",
        "        model_tokenclassification.zero_grad()\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        tr_acc += acc\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model_tokenclassification.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "    # VALIDATION on validation set\n",
        "    model_tokenclassification.eval()\n",
        "    for batch in val_dataloader:\n",
        "        batch = tuple(t for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            b_outputs = model_tokenclassification(b_input_ids, token_type_ids=None,\n",
        "                         attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            b_active_loss = b_input_mask.view(-1) == 1\n",
        "            b_active_logits = b_outputs.view(-1, config.num_labels)[b_active_loss]\n",
        "            b_active_labels = b_labels.view(-1)[b_active_loss]\n",
        "            loss = loss_fct(b_active_logits, b_active_labels)\n",
        "            acc = np.mean(np.argmax(b_active_logits.detach().cpu().numpy(), axis=1).flatten() == b_active_labels.detach().cpu().numpy().flatten())\n",
        "\n",
        "        eval_loss += loss.mean().item()\n",
        "        eval_acc += acc\n",
        "        nb_eval_steps += 1    \n",
        "    \n",
        "    if e % 10 ==0:\n",
        "        \n",
        "        print(f\"Epoch: {e}\")\n",
        "        print(f\"Train loss: {(tr_loss/nb_tr_steps)}\")\n",
        "        print(f\"Train acc: {(tr_acc/nb_tr_steps)}\")\n",
        "        print(f\"Train Time: {(time.time()-start_time)/60} mins\")  \n",
        "        \n",
        "        print(f\"Validation loss: {eval_loss/nb_eval_steps}\")\n",
        "        print(f\"Validation Accuracy: {(eval_acc/nb_eval_steps)}\") \n",
        "        \n",
        "        train_losses.append(tr_loss/nb_tr_steps)\n",
        "        train_accs.append(tr_acc/nb_tr_steps)\n",
        "        eval_losses.append(eval_loss/nb_eval_steps)\n",
        "        eval_accs.append(eval_acc/nb_eval_steps)\n",
        "        \n",
        "        \n",
        "        tr_loss, tr_acc, nb_tr_steps = 0, 0, 0 \n",
        "        eval_loss, eval_acc, nb_eval_steps = 0, 0, 0 \n",
        "        start_time = time.time() \n",
        "        \n",
        "        torch.save({'model_state_dict': model_tokenclassification.state_dict(),'optimizer_state_dict': optimizer.state_dict(),\n",
        "           'epoch': e, 'train_losses': train_losses,'train_accs': train_accs, 'eval_losses':eval_losses,'eval_accs':eval_accs}\n",
        "       , 'finetune_checkpoint')\n",
        "\n",
        "plot.plot(train_losses)\n",
        "plot.plot(train_accs)\n",
        "plot.plot(eval_losses)\n",
        "plot.plot(eval_accs)\n",
        "plot.legend(labels = ['train_loss','train_accuracy','validation_loss','validation_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Load from checkpoint ---\n",
            "--- Resume/Start training ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f37512a2f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngi6EED5CCroQQviI/w+obBcnxh/NrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-RyTUyGuM8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(texts):\n",
        "    tokenized_texts = [tokenizer.tokenize(txt) for txt in texts]\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_mask = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "    \n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, attention_mask)\n",
        "    datasampler = SequentialSampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=datasampler, batch_size=BATCH_SIZE) \n",
        "    \n",
        "    predicted_labels = []\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t for t in batch)\n",
        "        b_input_ids, b_input_mask = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model_tokenclassification(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "\n",
        "            predicted_labels.append(np.multiply(np.argmax(logits.detach().cpu().numpy(),axis=2), b_input_mask.detach().cpu().numpy()))\n",
        "    # np.concatenate(predicted_labels), to flatten list of arrays of batch_size * max_len into list of arrays of max_len\n",
        "    return np.concatenate(predicted_labels).astype(int), tokenized_texts\n",
        "\n",
        "def get_dish_candidate_names(predicted_label, tokenized_text):\n",
        "    name_lists = []\n",
        "    if len(np.where(predicted_label>0)[0])>0:\n",
        "        name_idx_combined = np.where(predicted_label>0)[0]\n",
        "        name_idxs = np.split(name_idx_combined, np.where(np.diff(name_idx_combined) != 1)[0]+1)\n",
        "        name_lists.append([\" \".join(np.take(tokenized_text,name_idx)) for name_idx in name_idxs])\n",
        "        # If there duplicate names in the name_lists\n",
        "        name_lists = np.unique(name_lists)\n",
        "        return name_lists\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "texts = df_data_val.review.values\n",
        "predicted_labels, _ = predict(texts)\n",
        "df_data_val['predicted_review_label'] = list(predicted_labels)\n",
        "df_data_val['predicted_name']=df_data_val.apply(lambda row: get_dish_candidate_names(row.predicted_review_label, row.review_tokens)\n",
        "                                                , axis=1)\n",
        "\n",
        "texts = df_data_train.review.values\n",
        "predicted_labels, _ = predict(texts)\n",
        "df_data_train['predicted_review_label'] = list(predicted_labels)\n",
        "df_data_train['predicted_name']=df_data_train.apply(lambda row: get_dish_candidate_names(row.predicted_review_label, row.review_tokens)\n",
        "                                                , axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhTzWEWiuePw",
        "colab_type": "code",
        "outputId": "d4542e43-c6e7-4477-9fe9-6701f0ab9ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "df_data_val"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>review</th>\n",
              "      <th>dish_name</th>\n",
              "      <th>name_tokens</th>\n",
              "      <th>review_tokens</th>\n",
              "      <th>review_label</th>\n",
              "      <th>predicted_review_label</th>\n",
              "      <th>predicted_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I like the chicken rice a lot!</td>\n",
              "      <td>chicken rice</td>\n",
              "      <td>[chicken, rice]</td>\n",
              "      <td>[i, like, the, chicken, rice, a, l, ##o, ##t, !]</td>\n",
              "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[chicken rice]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The mala steamboat doesn't taste nice.</td>\n",
              "      <td>mala steamboat</td>\n",
              "      <td>[mala, steamboat]</td>\n",
              "      <td>[the, mala, steamboat, d, ##o, ##e, ##s, ##n, ...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[mala steamboat]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...    predicted_name\n",
              "0      0  ...    [chicken rice]\n",
              "1      1  ...  [mala steamboat]\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0vRYSNiu6Hm",
        "colab_type": "code",
        "outputId": "3d5d1e65-369a-44bc-ed96-4a18e3f1264a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "df_data_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>dish_name</th>\n",
              "      <th>name_tokens</th>\n",
              "      <th>review_tokens</th>\n",
              "      <th>review_label</th>\n",
              "      <th>predicted_review_label</th>\n",
              "      <th>predicted_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I like the mala steamboat a lot!</td>\n",
              "      <td>mala steamboat</td>\n",
              "      <td>[mala, steamboat]</td>\n",
              "      <td>[i, like, the, mala, steamboat, a, l, ##o, ##t...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[mala steamboat]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The chicken rice doesn't taste nice.</td>\n",
              "      <td>chicken rice</td>\n",
              "      <td>[chicken, rice]</td>\n",
              "      <td>[the, chicken, rice, d, ##o, ##e, ##s, ##n, ',...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[chicken rice]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 review  ...    predicted_name\n",
              "0      I like the mala steamboat a lot!  ...  [mala steamboat]\n",
              "1  The chicken rice doesn't taste nice.  ...    [chicken rice]\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}